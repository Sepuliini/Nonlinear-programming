{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbce7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import ad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d2d32",
   "metadata": {},
   "source": [
    "<h2> Exercise 5 </h2>\n",
    "\n",
    "<break>\n",
    "<h4> Task 1 </h4>\n",
    "<p> In this task we will solve the optimization problem:: </p>\n",
    "\n",
    "$$  \\min \\qquad   x_1^2 + x_2^2 $$\n",
    "$$ \\qquad  \\text{s.t.}  \\qquad x_1 + x_2 ≥ 1 $$\n",
    "\n",
    "\n",
    "\n",
    "<p> We will solve the problem using just the optimality conditions </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1cabcf",
   "metadata": {},
   "source": [
    "\n",
    "Let us verify the KKT necessary conditions for the local minimum as:\n",
    "\n",
    "Let's first define a Lagrangian function as follows:\n",
    "\n",
    "$$ 𝐿(𝑥,𝜇,𝜆)= f(x)− 𝜇_1h(x) - 𝜆_1g(x) $$\n",
    "And we will get:\n",
    "$$ 𝐿(𝑥,𝜇,𝜆)=(x_1^2 + x_2^2) − 𝜇(0) - 𝜆(x_1 + x_2 - 1) $$\n",
    "So\n",
    "$$\n",
    "𝐿(𝑥,𝜇,𝜆) = x_1^2 + x_2^2 + λ - λx_1 - λx_2\n",
    "$$\n",
    "\n",
    "\n",
    "Calculating partial derivatives we will get:\n",
    "$$ \\frac{\\partial L}{\\partial x_1} = 2x_1 - 𝜆 = 0$$\n",
    "$$ $$\n",
    "$$ \\frac{\\partial L}{\\partial x_2} = 2x_2 - 𝜆 = 0$$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial 𝜆} = 1 - x_1 - x_2 = 0 $$\n",
    "$$ $$\n",
    "$$ $$\n",
    "From here we will get:\n",
    "$$ 2x_1 - 𝜆 = 0$$\n",
    "$$ 2x_2 - 𝜆 = 0$$\n",
    "So\n",
    "$$ x_1 = \\frac{𝜆}{2}$$\n",
    "$$ x_2 = \\frac{𝜆}{2}$$\n",
    "\n",
    "Thus\n",
    "$$ -1 + \\frac{𝜆}{2} + \\frac{𝜆}{2} = 0 $$\n",
    "$$ $$\n",
    "$$  \\frac{𝜆}{2} + \\frac{𝜆}{2} = 1 $$\n",
    "$$ $$\n",
    "$$  \\frac{2𝜆}{2} = 1 $$\n",
    "$$ $$\n",
    "$$ 𝜆 = 1 $$\n",
    "We get that $ 𝜆 > 0 $ \n",
    "$$ $$\n",
    "In this case, the inequality constraint is active and the optimal solution is on the boundary $x_1 + x_2 = 1$.\n",
    "\n",
    "So the optimal solution is $ x_1 = x_2 = \\frac{𝜆}{2}$ and the minimum value of the objective function is:.\n",
    "\n",
    " x* = [$\\frac{1}{2}$,$\\frac{1}{2}$]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603e918",
   "metadata": {},
   "source": [
    "<h4> Task 2 </h4>\n",
    "<p> In this task we consider a problem: </p>\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\min \\qquad & f(x) \\\\\n",
    " \\text{s.t.}\\qquad & h_k(x) = 0 \\\\\n",
    "  \\qquad & \\textit{for all } k = 1,...K \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "<p> and all of the functions are twice differentiable </p>\n",
    "<p> We need to show that the gradient of hte augmented Lagrangian function is zero in the minimizer $x^*$ of the above problem </p>\n",
    "<break>\n",
    "<p> In other words, let's show that: </p>\n",
    "    \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  & \\nabla_x L_c(x^*, \\lambda^*) = 0 \\\\\n",
    " & \\text{where } \\lambda^* \\in R^n \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "<p> is the corresponding optimal Lagrange multiplier vector </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fba526",
   "metadata": {},
   "source": [
    "We will define Lagrange function as follows:\n",
    "$$ L(𝑥,𝜆) = 𝑓(𝑥) + Σ_{𝑘=1}^𝐾 (𝜆_𝑘  ℎ_𝑘(𝑥)) + (\\frac{𝜇}{2}) Σ_{𝑘=1}^𝐾 (ℎ_𝑘(𝑥))^2 = 0$$\n",
    "\n",
    "Now we will take partial derivatives:\n",
    "$$\n",
    "\\frac{\\partial L(𝑥^*,𝜆^*)}{\\partial x} = \\frac{\\partial 𝑓(𝑥^*)}{\\partial x} + Σ_{𝑘=1}^𝐾 𝜆_𝑘^*  \\frac{\\partial ℎ_𝑘(𝑥^*)}{\\partial 𝑥} + 𝜇  Σ_{𝑘=1}^𝐾 ℎ_𝑘(𝑥^*)  \\frac{\\partial ℎ_𝑘(𝑥^*)}{\\partial x} = 0\n",
    "$$\n",
    "\n",
    "Since $𝑥^*$ is the minimizer of the optimization problem, we know that $ \\frac{\\partial 𝑓(𝑥^*)}{\\partial x}= 0$.\n",
    "\n",
    "We also know that $ \\frac{\\partial ℎ_𝑘(𝑥^*)}{\\partial 𝑥} = 0 $ for all $𝑘$ because $𝑥^*$ satisfies all the constraints.\n",
    "\n",
    "Therefore, the equation simplifies to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(𝑥^*,𝜆^*)}{\\partial x} =   𝜇 Σ_{𝑘=1}^𝐾 ℎ_𝑘(𝑥^*)  \\frac{\\partial ℎ_𝑘(𝑥^*)}{\\partial x} = 0\n",
    "$$\n",
    "\n",
    "Since $𝑥^*$ satisfies all the constraints, we know that $ℎ_𝑘(𝑥^*) = 0$ for all $𝑘$.\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  & \\nabla_x L_c(x^*, \\lambda^*) = 0 \\\\\n",
    " & \\text{where } \\lambda^* \\in R^n \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc41de",
   "metadata": {},
   "source": [
    "<h4> Task 3 </h4>\n",
    "<p> For tasks 3 and 4 we study the optimization problem: </p>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\min \\qquad & x_1^2 + x_2^2 + x_3^2 + (1-x_4)^2 \\\\\n",
    " \\text{s.t.}\\qquad & x_1^2 + x_2^2 -1 = 0 \\\\\n",
    "  \\qquad & x_1^2 + x_3^2 - 1 = 0 \\\\\n",
    "  \\qquad & x \\in R^4 \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f06cc7",
   "metadata": {},
   "source": [
    "<p> In this task we will be using the SQP method to solve above problem </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7018a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2 + x[2]**3 +(1 - x[3])**2, [], [x[0]**2 + x[1]**2 - 1, x[0]**2 + x[2]**2 - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbf01579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQP code from lectures:\n",
    "\n",
    "#if k=0, returns the gradient of lagrangian, if k=1, returns the hessian\n",
    "def diff_L(f,x,l,k):\n",
    "    #Define the lagrangian for given f and Lagrangian multiplier vector l\n",
    "    L = lambda x_: f(x_)[0] + (np.matrix(f(x_)[2])*np.matrix(l).transpose())[0,0]\n",
    "    return ad.gh(L)[k](x)\n",
    "\n",
    "#Returns the gradients of the equality constraints\n",
    "def grad_h(f,x):\n",
    "    return  [ad.gh(lambda y:\n",
    "                   f(y)[2][i])[0](x) for i in range(len(f(x)[2]))] \n",
    "\n",
    "#Solves the quadratic problem inside the SQP method\n",
    "def solve_QP(f,x,l):\n",
    "    left_side_first_row = np.concatenate((\\\n",
    "    np.matrix(diff_L(f,x,l,1)),\\\n",
    "    np.matrix(grad_h(f,x)).transpose()),axis=1)\n",
    "    left_side_second_row = np.concatenate((\\\n",
    "    np.matrix(grad_h(f,x)),\\\n",
    "    np.matrix(np.zeros((len(f(x)[2]),len(f(x)[2]))))),axis=1)\n",
    "    right_hand_side = np.concatenate((\\\n",
    "    -1*np.matrix(diff_L(f,x,l,0)).transpose(),\n",
    "    -np.matrix(f(x)[2]).transpose()),axis = 0)\n",
    "    left_hand_side = np.concatenate((\\\n",
    "                                    left_side_first_row,\\\n",
    "                                    left_side_second_row),axis = 0)\n",
    "    temp = np.linalg.solve(left_hand_side,right_hand_side)\n",
    "    return temp[:len(x)],temp[len(x):] # update for both x and l\n",
    "    \n",
    "def SQP(f,start,precision):\n",
    "    x = start\n",
    "    l = np.ones(len(f(x)[2])) # initialize Lagrange multiplier vector l as a vector of 1s\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x)[0]\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        print(x)\n",
    "        f_old = f_new\n",
    "        (p,v) = solve_QP(f,x,l) # obtain updates for x and l by solving the quadratic subproblem\n",
    "        x = x+np.array(p.transpose())[0] # update the solution x \n",
    "        l = l+v # update the Lagrange multipliers l\n",
    "        f_new = f(x)[0]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "627e1fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.01, 1.01, 1, 1]\n",
      "[0.8364941  0.66855541 0.66519096 1.        ]\n",
      "[0.96404623 0.39925692 0.39790335 1.        ]\n",
      "[1.06171489 0.05223047 0.05105227 1.        ]\n",
      "[1.00030513 0.05637356 0.05648277 1.        ]\n",
      "[0.99810267 0.06185432 0.06184383 1.        ]\n",
      "[0.99764957 0.06888367 0.06888486 1.        ]\n",
      "[0.99700931 0.07779635 0.07779619 1.        ]\n",
      "[0.99605277 0.08954185 0.08954187 1.        ]\n",
      "[0.99452175 0.10579737 0.10579736 1.        ]\n",
      "[0.99181601 0.12997203 0.12997204 1.        ]\n",
      "[0.98623211 0.17030639 0.17030639 1.        ]\n",
      "[0.97096141 0.2538701  0.2538701  1.        ]\n",
      "[0.88567195 0.56585931 0.56585931 1.        ]\n",
      "[ 1.80372195 -0.96349066 -0.96349066  1.        ]\n",
      "[ 1.160124   -0.51720525 -0.51720525  1.        ]\n",
      "[ 0.97990519 -0.32846333 -0.32846333  1.        ]\n",
      "[ 0.97287613 -0.24576503 -0.24576503  1.        ]\n",
      "[ 0.98216864 -0.19496581 -0.19496581  1.        ]\n",
      "[ 0.98781201 -0.15969712 -0.15969712  1.        ]\n",
      "[ 0.9912862  -0.13421317 -0.13421317  1.        ]\n",
      "[ 0.99353134 -0.11516637 -0.11516637  1.        ]\n",
      "[ 0.99504509 -0.10051049 -0.10051049  1.        ]\n",
      "[ 0.99610376 -0.0889498  -0.0889498   1.        ]\n",
      "[ 0.99686786 -0.07963546 -0.07963546  1.        ]\n",
      "[ 0.99743452 -0.07199369 -0.07199369  1.        ]\n",
      "[ 0.99786474 -0.06562551 -0.06562551  1.        ]\n",
      "[ 0.99819809 -0.06024639 -0.06024639  1.        ]\n",
      "[ 0.99846103 -0.05564879 -0.05564879  1.        ]\n",
      "[ 0.9986717  -0.05167822 -0.05167822  1.        ]\n",
      "[ 0.99884286 -0.04821763 -0.04821763  1.        ]\n",
      "[ 0.99898364 -0.04517687 -0.04517687  1.        ]\n",
      "[ 0.99910072 -0.0424855  -0.0424855   1.        ]\n",
      "[ 0.99919904 -0.04008777 -0.04008777  1.        ]\n",
      "[ 0.99928237 -0.03793899 -0.03793899  1.        ]\n",
      "[ 0.99935356 -0.03600299 -0.03600299  1.        ]\n",
      "[ 0.99941483 -0.03425019 -0.03425019  1.        ]\n",
      "[ 0.99946792 -0.03265618 -0.03265618  1.        ]\n",
      "[ 0.9995142  -0.03120065 -0.03120065  1.        ]\n",
      "[ 0.99955479 -0.02986657 -0.02986657  1.        ]\n",
      "[ 0.99959056 -0.02863956 -0.02863956  1.        ]\n",
      "[ 0.99962224 -0.0275074  -0.0275074   1.        ]\n",
      "[ 0.99965043 -0.02645966 -0.02645966  1.        ]\n",
      "[ 0.99967562 -0.02548733 -0.02548733  1.        ]\n",
      "[ 0.99969821 -0.02458268 -0.02458268  1.        ]\n",
      "[ 0.99971855 -0.02373894 -0.02373894  1.        ]\n",
      "[ 0.99973692 -0.02295024 -0.02295024  1.        ]\n",
      "[ 0.99975357 -0.02221142 -0.02221142  1.        ]\n",
      "res:  [ 0.9997687  -0.02151794 -0.02151794  1.        ]\n"
     ]
    }
   ],
   "source": [
    "x1 = [1,2,3,4]\n",
    "x2 = [1.01,1.01,1,1]\n",
    "#print(f(x))\n",
    "res = SQP(f, x2, 0.000001)\n",
    "print(\"res: \", res)\n",
    "\n",
    "#x2 = [0.0001, 1.9, 0.001, -1]\n",
    "#res = SQP(f, x2, 0.0001)\n",
    "#print(\"res: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da345f84",
   "metadata": {},
   "source": [
    "I tested this with couple different starting values and it seems like we would need a really good starting point\n",
    "to get accurate optimals using SQP. The function we are optimizing also has many local optimums but there is no global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49364f",
   "metadata": {},
   "source": [
    "<h4> Task 4 </h4>\n",
    "<p> In this task we will be using the Lagrangian method to solve the optimization problem </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3c967875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmented Lagrangian method from lectures:\n",
    "\n",
    "def augmented_langrangian(f,x,la,c):\n",
    "    second_term = float(numpy.matrix(la)*numpy.matrix(f(x)[2]).transpose())\n",
    "    third_term = 0.5*c*numpy.linalg.norm(f(x)[2])**2\n",
    "    return f(x)[0]+second_term+third_term\n",
    "\n",
    "def augmented_langrangian_method(f,start,la0,c0):\n",
    "    x_old = [float('inf')]*2\n",
    "    x_new = start\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x_new)[0]\n",
    "    la = la0\n",
    "    c = c0\n",
    "    steps = []\n",
    "    while abs(f_old-f_new)>0.00001:\n",
    "#    while numpy.linalg.norm(f(x_new)[2])>0.00001: # doesn't work as itself, see starting from any feasible point\n",
    "        res = minimize(lambda x:augmented_langrangian(f,x,la,c),x_new)\n",
    "        x_old = x_new\n",
    "        f_old = f_new\n",
    "        #la = float(la+numpy.matrix(c)*numpy.matrix(f(res.x)[2]).transpose())# update Lagrangian\n",
    "        #x = x+numpy.array(p.transpose())[0]\n",
    "        la = la0+c*numpy.matrix(f(res.x)[2])\n",
    "        x_new = res.x\n",
    "        f_new = f(x_new)[0]\n",
    "        c = 2*c # increase the penalty coefficient\n",
    "        steps.append(list(x_new))\n",
    "    return x_new,c, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9db251d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.91716640e-07  1.58942804e-07 -6.99961155e-09  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "x0 =[1,1,1,1]\n",
    "l0 = 1.0\n",
    "c0 = 1.0\n",
    "[x,c,steps_ag] = augmented_langrangian_method(f,x2,[10,10],c0)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3d2cd",
   "metadata": {},
   "source": [
    "I tested this method too with some different starting values. And it seems like this has same problems as SQP.\n",
    "It seems like we would need a really good starting point to get good optimal answer. Function also has many local minimas which makes interpretation of optimum difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eaf8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a055ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
