{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbce7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import ad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d2d32",
   "metadata": {},
   "source": [
    "<h2> Exercise 5 </h2>\n",
    "\n",
    "<break>\n",
    "<h4> Task 1 </h4>\n",
    "<p> In this task we will solve the optimization problem:: </p>\n",
    "\n",
    "$$  \\min \\qquad   x_1^2 + x_2^2 $$\n",
    "$$ \\qquad  \\text{s.t.}  \\qquad x_1 + x_2 ≥ 1 $$\n",
    "\n",
    "\n",
    "\n",
    "<p> We will solve the problem using just the optimality conditions </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1cabcf",
   "metadata": {},
   "source": [
    "\n",
    "Let us verify the KKT necessary conditions for the local minimum as:\n",
    "\n",
    "Let's first define a Lagrangian function as follows:\n",
    "\n",
    "$$ 𝐿(𝑥,𝜇,𝜆)= f(x)− 𝜇_1h(x) - 𝜆_1g(x) $$\n",
    "And we will get:\n",
    "$$ 𝐿(𝑥,𝜇,𝜆)=(x_1^2 + x_2^2) − 𝜇(0) - 𝜆(x_1 + x_2 - 1) $$\n",
    "So\n",
    "$$\n",
    "𝐿(𝑥,𝜇,𝜆) = x_1^2 + x_2^2 + λ - λx_1 - λx_2\n",
    "$$\n",
    "\n",
    "\n",
    "Calculating partial derivatives we will get:\n",
    "$$ \\frac{\\partial L}{\\partial x_1} = 2x_1 - 𝜆 = 0$$\n",
    "$$ $$\n",
    "$$ \\frac{\\partial L}{\\partial x_2} = 2x_2 - 𝜆 = 0$$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial 𝜆} = 1 - x_1 - x_2 = 0 $$\n",
    "$$ $$\n",
    "$$ $$\n",
    "From here we will get:\n",
    "$$ 2x_1 - 𝜆 = 0$$\n",
    "$$ 2x_2 - 𝜆 = 0$$\n",
    "So\n",
    "$$ x_1 = \\frac{𝜆}{2}$$\n",
    "$$ x_2 = \\frac{𝜆}{2}$$\n",
    "\n",
    "Thus\n",
    "$$ -1 + \\frac{𝜆}{2} + \\frac{𝜆}{2} = 0 $$\n",
    "$$ $$\n",
    "$$  \\frac{𝜆}{2} + \\frac{𝜆}{2} = 1 $$\n",
    "$$ $$\n",
    "$$  \\frac{2𝜆}{2} = 1 $$\n",
    "$$ $$\n",
    "$$ 𝜆 = 1 $$\n",
    "We get that $ 𝜆 > 0 $ \n",
    "$$ $$\n",
    "In this case, the inequality constraint is active and the optimal solution is on the boundary $x_1 + x_2 = 1$.\n",
    "\n",
    "So the optimal solution is $ x_1 = x_2 = \\frac{𝜆}{2}$ and the minimum value of the objective function is:.\n",
    "\n",
    " x* = [$\\frac{1}{2}$,$\\frac{1}{2}$]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603e918",
   "metadata": {},
   "source": [
    "<h4> Task 2 </h4>\n",
    "<p> In this task we consider a problem: </p>\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\min \\qquad & f(x) \\\\\n",
    " \\text{s.t.}\\qquad & h_k(x) = 0 \\\\\n",
    "  \\qquad & \\textit{for all } k = 1,...K \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "<p> and all of the functions are twice differentiable </p>\n",
    "<p> We need to show that the gradient of hte augmented Lagrangian function is zero in the minimizer $x^*$ of the above problem </p>\n",
    "<break>\n",
    "<p> In other words, let's show that: </p>\n",
    "    \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  & \\nabla_x L_c(x^*, \\lambda^*) = 0 \\\\\n",
    " & \\text{where } \\lambda^* \\in R^n \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "    <p> is the corresponding optimal Lagrange multiplier vector </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fba526",
   "metadata": {},
   "source": [
    "We will define Lagrange function as follows:\n",
    "$$ L(𝑥,𝜆) = 𝑓(𝑥) + Σ_{𝑘=1}^𝐾 (𝜆_𝑘  ℎ_𝑘(𝑥)) + (\\frac{𝜇}{2}) Σ_{𝑘=1}^𝐾 (ℎ_𝑘(𝑥))^2 = 0$$\n",
    "\n",
    "Now we will take partial derivatives:\n",
    "$$\n",
    "\\frac{\\partial L(𝑥^*,𝜆^*)}{\\partial x} = \\frac{\\partial 𝑓(𝑥^*)}{\\partial x} + Σ_{𝑘=1}^𝐾 𝜆_𝑘^*  \\frac{\\partial ℎ_𝑘(𝑥^*)}{\\partial 𝑥} + 𝜇  Σ_{𝑘=1}^𝐾 ℎ_𝑘(𝑥^*)  \\frac{\\partial ℎ_𝑘(𝑥^*)}{\\partial x} = 0\n",
    "$$\n",
    "\n",
    "Since $𝑥^*$ is the minimizer of the optimization problem, we know that $ \\frac{\\partial 𝑓(𝑥^*)}{\\partial x}= 0$.\n",
    "\n",
    "We also know that $ \\frac{\\partial ℎ_𝑘(𝑥^*)}{\\partial 𝑥} = 0 $ for all $𝑘$ because $𝑥^*$ satisfies all the constraints.\n",
    "\n",
    "Therefore, the equation simplifies to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(𝑥^*,𝜆^*)}{\\partial x} =   𝜇 Σ_{𝑘=1}^𝐾 ℎ_𝑘(𝑥^*)  \\frac{\\partial ℎ_𝑘(𝑥^*)}{\\partial x} = 0\n",
    "$$\n",
    "\n",
    "Since $𝑥^*$ satisfies all the constraints, we know that $ℎ_𝑘(𝑥^*) = 0$ for all $𝑘$.\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  & \\nabla_x L_c(x^*, \\lambda^*) = 0 \\\\\n",
    " & \\text{where } \\lambda^* \\in R^n \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc41de",
   "metadata": {},
   "source": [
    "<h4> Task 3 </h4>\n",
    "<p> For tasks 3 and 4 we study the optimization problem: </p>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "  \\min \\qquad & x_1^2 + x_2^2 + x_3^2 + (1-x_4)^2 \\\\\n",
    " \\text{s.t.}\\qquad & x_1^2 + x_2^2 -1 = 0 \\\\\n",
    "  \\qquad & x_1^2 + x_3^2 - 1 = 0 \\\\\n",
    "  \\qquad & x \\in R^4 \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f06cc7",
   "metadata": {},
   "source": [
    "<p> In this task we will be using the SQP method to solve above problem </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7018a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2 + x[2]**2 +(1 - x[3])**2, [], [x[0]**2 + x[1]**2 - 1, x[0]**2 + x[2]**2 - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbf01579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQP code from lectures:\n",
    "\n",
    "#if k=0, returns the gradient of lagrangian, if k=1, returns the hessian\n",
    "def diff_L(f,x,l,k):\n",
    "    #Define the lagrangian for given f and Lagrangian multiplier vector l\n",
    "    L = lambda x_: f(x_)[0] + (np.matrix(f(x_)[2])*np.matrix(l).transpose())[0,0]\n",
    "    return ad.gh(L)[k](x)\n",
    "\n",
    "#Returns the gradients of the equality constraints\n",
    "def grad_h(f,x):\n",
    "    return  [ad.gh(lambda y:\n",
    "                   f(y)[2][i])[0](x) for i in range(len(f(x)[2]))] \n",
    "\n",
    "#Solves the quadratic problem inside the SQP method\n",
    "def solve_QP(f,x,l):\n",
    "    left_side_first_row = np.concatenate((\\\n",
    "    np.matrix(diff_L(f,x,l,1)),\\\n",
    "    np.matrix(grad_h(f,x)).transpose()),axis=1)\n",
    "    left_side_second_row = np.concatenate((\\\n",
    "    np.matrix(grad_h(f,x)),\\\n",
    "    np.matrix(np.zeros((len(f(x)[2]),len(f(x)[2]))))),axis=1)\n",
    "    right_hand_side = np.concatenate((\\\n",
    "    -1*np.matrix(diff_L(f,x,l,0)).transpose(),\n",
    "    -np.matrix(f(x)[2]).transpose()),axis = 0)\n",
    "    left_hand_side = np.concatenate((\\\n",
    "                                    left_side_first_row,\\\n",
    "                                    left_side_second_row),axis = 0)\n",
    "    temp = np.linalg.solve(left_hand_side,right_hand_side)\n",
    "    return temp[:len(x)],temp[len(x):] # update for both x and l\n",
    "    \n",
    "def SQP(f,start,precision):\n",
    "    x = start\n",
    "    l = np.ones(len(f(x)[2])) # initialize Lagrange multiplier vector l as a vector of 1s\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x)[0]\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        print(x)\n",
    "        f_old = f_new\n",
    "        (p,v) = solve_QP(f,x,l) # obtain updates for x and l by solving the quadratic subproblem\n",
    "        x = x+np.array(p.transpose())[0] # update the solution x \n",
    "        l = l+v # update the Lagrange multipliers l\n",
    "        f_new = f(x)[0]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "627e1fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 1.5, 0.1, 1]\n",
      "[0.33924329 1.08110504 5.01657567 1.        ]\n",
      "[1.62688451 0.54576326 2.50941078 1.        ]\n",
      "[ 1.5677837  -1.05961264  0.96490572  1.        ]\n",
      "[ 1.23098012 -0.34017315  0.27420692  1.        ]\n",
      "[ 1.05269127 -0.057832   -0.00215631  1.        ]\n",
      "[ 1.00132387 -0.02882187  0.00144634  1.        ]\n",
      "[ 1.00000785 -0.01416875 -0.00410297  1.        ]\n",
      "[1.00011732 0.0011959  0.02654271 1.        ]\n",
      "[ 1.00000983 -0.00762093  0.01290105  1.        ]\n",
      "[ 1.00000301 -0.00341611  0.00621757  1.        ]\n",
      "res:  [1.00000842e+00 7.55691872e-04 1.75513131e-03 1.00000000e+00]\n",
      "[0.0001, 1.9, 0.001, -1]\n",
      "[ 33.11265845   1.21141513 496.68923916   1.        ]\n",
      "[ 1.66019634e+01 -2.28910831e-01  2.48342584e+02  1.00000000e+00]\n",
      "[8.33292475e+00 1.79858635e-02 1.24171170e+02 1.00000000e+00]\n",
      "[ 4.22648594e+00 -5.62332449e-04  6.20855836e+01  1.00000000e+00]\n",
      "[ 2.23154460e+00 -2.90548367e-06  3.10427918e+01  1.00000000e+00]\n",
      "[ 1.33983235e+00 -2.06866592e-07  1.55213959e+01  1.00000000e+00]\n",
      "[ 1.04309719e+00 -3.68375062e-08  7.76069795e+00  1.00000000e+00]\n",
      "[ 1.00089031e+00 -3.72114503e-09  3.88034897e+00  1.00000000e+00]\n",
      "[1.00000040e+00 1.05718860e-10 1.94017449e+00 1.00000000e+00]\n",
      "[1.00000000e+00 1.50338633e-12 9.70087243e-01 1.00000000e+00]\n",
      "[ 1.00000000e+00 -1.08434268e-14  4.85043622e-01  1.00000000e+00]\n",
      "[ 1.00000000e+00 -3.88249883e-17  2.42521811e-01  1.00000000e+00]\n",
      "[1.00000000e+00 6.97563750e-20 1.21260905e-01 1.00000000e+00]\n",
      "[1.00000000e+00 6.25528192e-23 6.06304527e-02 1.00000000e+00]\n",
      "[ 1.00000000e+00 -2.80717506e-26  3.03152263e-02  1.00000000e+00]\n",
      "[ 1.00000000e+00 -6.29603677e-30  1.51576132e-02  1.00000000e+00]\n",
      "[1.00000000e+00 7.06207679e-34 7.57880659e-03 1.00000000e+00]\n",
      "res:  [1.00000000e+00 3.96021631e-38 3.78940329e-03 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "x1 = [0.01, 1.5, 0.1, 1]\n",
    "#print(f(x))\n",
    "res = SQP(f, x1, 0.0001)\n",
    "print(\"res: \", res)\n",
    "\n",
    "x2 = [0.0001, 1.9, 0.001, -1]\n",
    "res = SQP(f, x2, 0.0001)\n",
    "print(\"res: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da345f84",
   "metadata": {},
   "source": [
    "I tested this with couple different starting values and it seems like we would need a really good starting point\n",
    "to get accurate optimals using SQP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49364f",
   "metadata": {},
   "source": [
    "<h4> Task 4 </h4>\n",
    "<p> In this task we will be using the Lagrangian method to solve the optimization problem </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c967875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmented Lagrangian method from lectures:\n",
    "\n",
    "def augmented_langrangian(f,x,la,c):\n",
    "    second_term = float(numpy.matrix(la)*numpy.matrix(f(x)[2]).transpose())\n",
    "    third_term = 0.5*c*numpy.linalg.norm(f(x)[2])**2\n",
    "    return f(x)[0]+second_term+third_term\n",
    "\n",
    "def augmented_langrangian_method(f,start,la0,c0):\n",
    "    x_old = [float('inf')]*2\n",
    "    x_new = start\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x_new)[0]\n",
    "    la = la0\n",
    "    c = c0\n",
    "    steps = []\n",
    "    while abs(f_old-f_new)>0.00001:\n",
    "#    while numpy.linalg.norm(f(x_new)[2])>0.00001: # doesn't work as itself, see starting from any feasible point\n",
    "        res = minimize(lambda x:augmented_langrangian(f,x,la,c),x_new)\n",
    "        x_old = x_new\n",
    "        f_old = f_new\n",
    "        la = float(la+numpy.matrix(c)*numpy.matrix(f(res.x)[2]).transpose()) # update Lagrangian\n",
    "        x_new = res.x\n",
    "        f_new = f(x_new)[0]\n",
    "        c = 2*c # increase the penalty coefficient\n",
    "        steps.append(list(x_new))\n",
    "    return x_new,c, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9db251d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 1.5, 0.1, 1]\n",
      "[0.33924329 1.08110504 5.01657567 1.        ]\n",
      "[1.62688451 0.54576326 2.50941078 1.        ]\n",
      "[ 1.5677837  -1.05961264  0.96490572  1.        ]\n",
      "[ 1.23098012 -0.34017315  0.27420692  1.        ]\n",
      "[ 1.05269127 -0.057832   -0.00215631  1.        ]\n",
      "[ 1.00132387 -0.02882187  0.00144634  1.        ]\n",
      "[ 1.00000785 -0.01416875 -0.00410297  1.        ]\n",
      "[1.00011732 0.0011959  0.02654271 1.        ]\n",
      "[ 1.00000983 -0.00762093  0.01290105  1.        ]\n",
      "[ 1.00000301 -0.00341611  0.00621757  1.        ]\n",
      "res:  [1.00000842e+00 7.55691872e-04 1.75513131e-03 1.00000000e+00]\n",
      "[0.0001, 1.9, 0.001, -1]\n",
      "[ 33.11265845   1.21141513 496.68923916   1.        ]\n",
      "[ 1.66019634e+01 -2.28910831e-01  2.48342584e+02  1.00000000e+00]\n",
      "[8.33292475e+00 1.79858635e-02 1.24171170e+02 1.00000000e+00]\n",
      "[ 4.22648594e+00 -5.62332449e-04  6.20855836e+01  1.00000000e+00]\n",
      "[ 2.23154460e+00 -2.90548367e-06  3.10427918e+01  1.00000000e+00]\n",
      "[ 1.33983235e+00 -2.06866592e-07  1.55213959e+01  1.00000000e+00]\n",
      "[ 1.04309719e+00 -3.68375062e-08  7.76069795e+00  1.00000000e+00]\n",
      "[ 1.00089031e+00 -3.72114503e-09  3.88034897e+00  1.00000000e+00]\n",
      "[1.00000040e+00 1.05718860e-10 1.94017449e+00 1.00000000e+00]\n",
      "[1.00000000e+00 1.50338633e-12 9.70087243e-01 1.00000000e+00]\n",
      "[ 1.00000000e+00 -1.08434268e-14  4.85043622e-01  1.00000000e+00]\n",
      "[ 1.00000000e+00 -3.88249883e-17  2.42521811e-01  1.00000000e+00]\n",
      "[1.00000000e+00 6.97563750e-20 1.21260905e-01 1.00000000e+00]\n",
      "[1.00000000e+00 6.25528192e-23 6.06304527e-02 1.00000000e+00]\n",
      "[ 1.00000000e+00 -2.80717506e-26  3.03152263e-02  1.00000000e+00]\n",
      "[ 1.00000000e+00 -6.29603677e-30  1.51576132e-02  1.00000000e+00]\n",
      "[1.00000000e+00 7.06207679e-34 7.57880659e-03 1.00000000e+00]\n",
      "res:  [1.00000000e+00 3.96021631e-38 3.78940329e-03 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "x1 = [0.01, 1.5, 0.1, 1]\n",
    "#print(f(x))\n",
    "res = SQP(f, x1, 0.0001)\n",
    "print(\"res: \", res)\n",
    "\n",
    "x2 = [0.0001, 1.9, 0.001, -1]\n",
    "res = SQP(f, x2, 0.0001)\n",
    "print(\"res: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3d2cd",
   "metadata": {},
   "source": [
    "I tested this method too with some different starting values. And it seems like this has same problems as SQP.\n",
    "It seems like we would need a really good starting point to get good optimal answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eaf8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
